{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7238e0ec",
   "metadata": {},
   "source": [
    "# Data import and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d78d123-43d5-4133-90de-1cf0fd6d29ec",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e355769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import warnings\n",
    "from datetime import date, datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import optimize, stats\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import argrelmax, find_peaks, hilbert, peak_widths\n",
    "from scipy.stats import norm\n",
    "from tabulate import tabulate\n",
    "from tkinter import filedialog, Tk\n",
    "from uncertainties import ufloat, unumpy\n",
    "from uncertainties.umath import *\n",
    "\n",
    "from lmfit import Minimizer, Model, Parameters, report_fit\n",
    "from lmfit.models import LinearModel, SplitLorentzianModel\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb26487-b1f1-40a2-a94f-ca76fed6a543",
   "metadata": {},
   "source": [
    "### Selecting data folder\n",
    "#### Creates result folder for output saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11340b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Tk()\n",
    "root.withdraw()\n",
    "folder_path = filedialog.askdirectory()\n",
    "print('Folder path: ', folder_path)\n",
    "\n",
    "file_path = os.path.join(folder_path, '**/*.csv')\n",
    "result_folder = 'res'\n",
    "folder = os.path.join(folder_path, result_folder)\n",
    "\n",
    "if os.path.isdir(folder):\n",
    "    print('Result folder already exists, cleaning...\\n (If an error occurs, consider remove Result folder manually.)')\n",
    "    try:\n",
    "        shutil.rmtree(folder)\n",
    "    except OSError as e:\n",
    "        print(f\"Error: {folder} : {e.strerror}\")\n",
    "os.mkdir(folder)\n",
    "print('Result folder created.')\n",
    "\n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "print('Result folder path: ', folder)\n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "\n",
    "### Logging\n",
    "log_file = os.path.join(folder, 'log.txt')\n",
    "\n",
    "with open(log_file, 'w') as f:\n",
    "    f.write(f\"{datetime.now()} - {folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f4bd1a-a1e9-4e19-b3e2-606fad3d5353",
   "metadata": {},
   "source": [
    "### Acquiring individual files addresses \n",
    "#### Splits into group, sensor and time, adding a hash column used for filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47facbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = pd.DataFrame(glob.glob(file_path,recursive = True),columns = ['Path'])\n",
    "\n",
    "def extract_info(path):\n",
    "    group = path.split('/')[-1].split('\\\\')[-3]\n",
    "    sensor = path.split('/')[-1].split('\\\\')[-2]\n",
    "    k = path.rfind('\\\\')\n",
    "    time = int(path[k + 1:].replace('.CSV', ''))\n",
    "    hash = f\"{group}-{sensor}-{time}\"\n",
    "    return pd.Series([group, sensor, time, hash])\n",
    "\n",
    "files[['group', 'sensor', 'time', 'hash']] = files['Path'].apply(extract_info)\n",
    "\n",
    "addresses = files.rename(columns={'Path': 'address'}).sort_values(\n",
    "    by=['group','sensor','time','address'], \n",
    "    ignore_index=True, \n",
    "    ascending=True\n",
    ")\n",
    "\n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "\n",
    "text = 'Analysis'\n",
    "\n",
    "print(text + '\\n')\n",
    "\n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "\n",
    "addresses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb654d-5b6e-4373-bfab-2508db4cee88",
   "metadata": {},
   "source": [
    "#### Provides intel on quantity of files to be scanned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = len(addresses.index)\n",
    "print('Space: ', space)\n",
    "today = date.today()\n",
    "\n",
    "### Logging\n",
    "\n",
    "files_address_list = f'{datetime.now()} - Address list - OK \\n'\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(files_address_list)\n",
    "\n",
    "addresses = addresses.reset_index(drop=True).set_index('address')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeff516-a409-4ff3-b541-5d872e5e2147",
   "metadata": {},
   "source": [
    "### Scans the addresses data frame, reads data of each file and assembles them into a combined data frame, that contains frequency and signal data\n",
    "### Also, rescales frequency values to signal's order of magnitude (multiplied by $1 \\times 10^{-6}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103861ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(path):\n",
    "    group = addresses.loc[path,'group']\n",
    "    sensor = addresses.loc[path,'sensor']\n",
    "    time = addresses.loc[path,'time']\n",
    "    hashed = addresses.loc[path,'hash']\n",
    "\n",
    "    df_import = pd.DataFrame(pd.read_csv(path, skiprows=range(0, 2)))\n",
    "    df_import.drop(df_import.columns[2], axis=1, inplace=True)\n",
    "    df_import.columns.values[0] = 'frequency'\n",
    "    df_import.columns.values[1] = 'signal'\n",
    "    df_import['group'] = group\n",
    "    df_import['sensor'] = sensor                                          \n",
    "    df_import['time'] = time\n",
    "    df_import['hash'] = hashed\n",
    "\n",
    "    return df_import\n",
    "\n",
    "dfs = addresses.index.map(process_path)\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "combined_df['frequency'] = combined_df['frequency'] * 1.e-6\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a3dbaa-91d8-4106-8a31-eb9fc869a787",
   "metadata": {},
   "source": [
    "### Exports combined and addresses data frames to results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5397b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_export = f'{str(datetime.now())} - Combined and address datasets - EXPORTED \\n'\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(datasets_export)\n",
    "    f.close()\n",
    "    \n",
    "combined_df.to_csv(folder + '/combined_df.csv', sep=';', index=False)\n",
    "addresses.to_csv(folder + '/addresses.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d7ffe7",
   "metadata": {},
   "source": [
    "# Defining functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0bd80",
   "metadata": {},
   "source": [
    "### Split Lorentzian funcion\n",
    "#### Function used to fit data to model\n",
    "\n",
    "$$\n",
    "f(x; A, \\mu, \\sigma, \\sigma_r) = \\frac{2 A}{\\pi (\\sigma+\\sigma_r)} \\big[\\frac{\\sigma^2}{(x - \\mu)^2 + \\sigma^2} * H(\\mu-x) + \\frac{\\sigma_r^2}{(x - \\mu)^2 + \\sigma_r^2} * H(x-\\mu)\\big] + (m x + b)\n",
    "$$\n",
    "\n",
    "### Applies fitting model (Lorentzian) to each experimental dataset, retrieving a evaluation dataset based on the model. After fitting, applies an optimization function over a Monte Carlo Simulation to find the interesting point: the minimized frequency used to evaluate delocation on passing time of experiment\n",
    "\n",
    "\n",
    "#### For each instance of time, plots original data, accuracy rate between Monte Carlo Simulation average frequency and minimized average frequency, and fitted curve over original data, pinpointing those values alongside minimal point from original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a28980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitLorentzianFunc(x, aux):\n",
    "    # Calculate the amplitude of the Lorentzian function\n",
    "    amplitude = ((2 * aux[0]) / (np.pi * (aux[2] + aux[3])))\n",
    "    \n",
    "    # Calculate the Lorentzian function for the lower and upper halves\n",
    "    lower_half = ((aux[2] ** 2) / (((x - aux[1]) ** 2) + aux[2] ** 2))\n",
    "    upper_half = ((aux[3] ** 2) / (((x - aux[1]) ** 2) + aux[3] ** 2))\n",
    "    \n",
    "    # Calculate the Heaviside step function for the lower and upper halves\n",
    "    step_lower = np.heaviside(aux[1] - x, 0)\n",
    "    step_upper = np.heaviside(x - aux[1], 0)\n",
    "    \n",
    "    # Calculate the linear function\n",
    "    linear_func = (aux[4] * x + aux[5])\n",
    "\n",
    "    # Return the sum of the Lorentzian function and the linear function\n",
    "    return (amplitude * ((lower_half * step_lower) + (upper_half * step_upper))) + linear_func\n",
    "\n",
    "def create_evaluate_df(x_eval, y_eval, hashed):\n",
    "    evaluate_df = pd.DataFrame(x_eval, columns=['x_eval'])\n",
    "    evaluate_df['y_eval'] = y_eval\n",
    "    evaluate_df['hash'] = hashed\n",
    "    return evaluate_df\n",
    "\n",
    "\n",
    "def process_data(combined_df, log_file):\n",
    "    unique_hash = combined_df.hash.unique()\n",
    "    dfs_params = []\n",
    "    dfs_eval = []\n",
    "    frequency_shift = []\n",
    "\n",
    "    for i in unique_hash:\n",
    "        hashed = i\n",
    "        xy0 = combined_df[['frequency','signal']].loc[combined_df['hash'] == hashed]\n",
    "        xy0 = xy0.reset_index(drop=True)\n",
    "\n",
    "        interval = 13\n",
    "        min_value_idx = xy0.loc[xy0['signal'] == xy0['signal'].min()].index[0]\n",
    "        idx = range(min_value_idx - interval, min_value_idx + interval)\n",
    "\n",
    "        x = np.array(xy0['frequency'])[idx]\n",
    "        y = np.array(xy0['signal'])[idx]\n",
    "        \n",
    "        # Model parametrization\n",
    "        peak = SplitLorentzianModel() #prefix='slm_')\n",
    "        linemod1 = LinearModel() #prefix='lm1_')\n",
    "        #linemod2 = LinearModel(prefix='lm2_')\n",
    "        pars = Parameters()\n",
    "        pars += peak.guess(y, x=x)\n",
    "        pars += linemod1.make_params(intercept=y.min(), slope=0)\n",
    "        #pars += linemod2.make_params(intercept=y.min(), slope=0)\n",
    "        mod = linemod1 + peak # + linemod2\n",
    "\n",
    "        # Fit model\n",
    "        result = mod.fit(y, pars, x=x)\n",
    "        \n",
    "        # Printing report and related information\n",
    "        print(result.fit_report(min_correl=0.25))\n",
    "\n",
    "        ### Logging\n",
    "\n",
    "        lmfit_report = f'\\n{str(datetime.now())} - LMFIT result report {i} \\n' + str(result.fit_report(min_correl=0.25))\n",
    "\n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(lmfit_report)\n",
    "        \n",
    "        # Determining the point to be evaluated on the frequency shift   \n",
    "        x_eval = np.linspace(min(x), max(x), num = 10000)\n",
    "        y_eval = result.eval(result.params, x=x_eval)\n",
    "\n",
    "        evaluate_df = pd.DataFrame(x_eval, columns=['x_eval'])\n",
    "        evaluate_df['y_eval'] = y_eval\n",
    "        evaluate_df['hash'] = hashed\n",
    "        \n",
    "        plot_x = evaluate_df['x_eval'].loc[evaluate_df['y_eval'] == evaluate_df['y_eval'].min()]\n",
    "        plot_y = evaluate_df['y_eval'].loc[evaluate_df['y_eval'] == evaluate_df['y_eval'].min()]\n",
    "\n",
    "        # Min point\n",
    "        minimized_freq = []\n",
    "\n",
    "        n = 1000\n",
    "        \n",
    "        for j in range(n):\n",
    "            aux = np.array([\n",
    "                np.random.normal( # 0\n",
    "                    loc=result.params['amplitude'].value, \n",
    "                    scale=result.params['amplitude'].stderr\n",
    "                ),\n",
    "                np.random.normal( # 1\n",
    "                    loc=result.params['center'].value, \n",
    "                    scale=result.params['center'].stderr\n",
    "                ), \n",
    "                np.random.normal( # 2\n",
    "                    loc=result.params['sigma'].value, \n",
    "                    scale=result.params['sigma'].stderr\n",
    "                ),\n",
    "                np.random.normal( # 3\n",
    "                    loc=result.params['sigma_r'].value, \n",
    "                    scale=result.params['sigma_r'].stderr\n",
    "                ),\n",
    "                np.random.normal( # 4\n",
    "                    loc=result.params['slope'].value, \n",
    "                    scale=result.params['slope'].stderr\n",
    "                ),\n",
    "                np.random.normal( # 5\n",
    "                    loc=result.params['intercept'].value, \n",
    "                    scale=result.params['intercept'].stderr\n",
    "                    )\n",
    "                ])\n",
    "\n",
    "            find_fmin = optimize.fmin(\n",
    "                lambda x: SplitLorentzianFunc(x, aux),\n",
    "                xy0['frequency'][min_value_idx], \n",
    "                full_output=True,\n",
    "                disp=0\n",
    "            )\n",
    "    \n",
    "            find_fmin_point = np.array([find_fmin[0].item(), find_fmin[1]])\n",
    "    \n",
    "            minimized_freq.append(find_fmin[0])\n",
    "    \n",
    "            j = j + 1\n",
    "\n",
    "        minimized_freq = np.concatenate(minimized_freq).ravel()\n",
    "        minimized_freq = minimized_freq[(minimized_freq > 0.43) & (minimized_freq < 0.444)]\n",
    "        \n",
    "        minimized_freq_mean = np.array(minimized_freq).mean()\n",
    "        minimized_freq_std = np.array(minimized_freq).std()\n",
    "        minimized_freq_std_err = minimized_freq_std / np.sqrt(n)\n",
    "        freq = ufloat(minimized_freq_mean,minimized_freq_std)*1e6\n",
    "        pfloat = minimized_freq_mean/plot_x.values.item()\n",
    "        \n",
    "        values_lst = [\n",
    "            minimized_freq_mean, \n",
    "            minimized_freq_std, \n",
    "            minimized_freq_std_err,\n",
    "            freq,\n",
    "            hashed\n",
    "        ]\n",
    "        \n",
    "        dfs_params.append(values_lst)\n",
    "        \n",
    "        print('\\n')\n",
    "        print('----- Results -----')\n",
    "        \n",
    "        def create_result_table(minimized_freq_mean, minimized_freq_std, minimized_freq_std_err, freq, pfloat):\n",
    "            result_table = [\n",
    "                ['Optimized frequency mean (fmin)', minimized_freq_mean],\n",
    "                ['Optimized frequency standard deviation (fmin)', minimized_freq_std],\n",
    "                ['Optimized frequency standard error (fmin)', minimized_freq_std_err],\n",
    "                ['Optimized frequency mean with uncertainties', freq],\n",
    "                ['Accuracy of estimated frequency mean / SMC',pfloat]\n",
    "            ]\n",
    "            return result_table\n",
    "        \n",
    "        result_table = create_result_table(minimized_freq_mean, minimized_freq_std, minimized_freq_std_err, freq, pfloat)\n",
    "        print(str(tabulate(result_table)))\n",
    "        print('\\n')\n",
    "        \n",
    "        ### Logging\n",
    "        \n",
    "        variables_report = f'{datetime.now()} - Results {i} \\n' + str(tabulate(result_table)) + '\\n'\n",
    "        \n",
    "        with open(log_file, 'a') as f:\n",
    "            f.write(variables_report)\n",
    "            \n",
    "        # PLOTING\n",
    "    \n",
    "        # Primary data\n",
    "        plt.figure()\n",
    "        plt.rcParams.update({'font.size': 18})\n",
    "        ax = xy0.plot(\n",
    "            x = 'frequency', \n",
    "            y = 'signal', \n",
    "            kind='scatter',\n",
    "            figsize = (16,4), \n",
    "            grid=True, \n",
    "            legend=True\n",
    "        )\n",
    "        ax.set_title(label = 'Initial data ' + hashed, pad=20, fontdict={'fontsize':20})\n",
    "        ax.set_xlabel('Frequency [MHz]')\n",
    "        ax.set_ylabel('Signal')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "        \n",
    "        #Accuracy between minimized frequency mean and MCS frequency mean\n",
    "        xfloat = np.linspace(0.98, 1.02, num = 100)\n",
    "        yfloat = np.linspace(0, 0, num = 100)\n",
    "        fig = plt.figure(figsize = (16,4))\n",
    "        plt.plot(xfloat,yfloat)\n",
    "        plt.plot(pfloat,0,color='k',marker='|', markersize = 15, label='Optimized frequency mean LMFIT / Optimized frequency mean SMC')\n",
    "        plt.text(\n",
    "            x=pfloat, \n",
    "            y=0.02, \n",
    "            s='Accuracy:    {:.8}'.format(pfloat), \n",
    "            horizontalalignment='right',\n",
    "            verticalalignment='baseline'\n",
    "        ) \n",
    "        plt.legend(loc='best')\n",
    "        plt.title('Accuracy between frequency mean LMFIT and SMC optimized frequency mean', fontsize=20)\n",
    "        plt.show()\n",
    "        \n",
    "        # Fit model data plot    \n",
    "        fig = plt.figure(figsize = (16,8))\n",
    "        plt.plot(x, y, 'o')\n",
    "        plt.plot(x_eval, y_eval, 'r-', label='Best fit')\n",
    "    \n",
    "        plt.plot(\n",
    "            xy0['frequency'][min_value_idx],\n",
    "            xy0['signal'][min_value_idx],\n",
    "            marker = 'D',\n",
    "            color='orange', \n",
    "            markersize=8,\n",
    "            label='Original data minimum frequency'\n",
    "        )\n",
    "        \n",
    "        labels = evaluate_df['hash'].loc[evaluate_df['y_eval'] == evaluate_df['y_eval'].min()]\n",
    "        \n",
    "        plt.plot(\n",
    "            plot_x, \n",
    "            plot_y, \n",
    "            label='Lorentz minimum frequency (LMFIT)', \n",
    "            color='green', \n",
    "            marker='s', \n",
    "            markersize=8\n",
    "        )\n",
    "        plt.plot(\n",
    "            minimized_freq_mean, \n",
    "            plot_y, \n",
    "            label='SMC average frequency', \n",
    "            color='k', \n",
    "            marker='o', \n",
    "            markersize=8\n",
    "        )\n",
    "        plt.xlabel('Frequency [MHz]')\n",
    "        plt.ylabel('Signal')\n",
    "        plt.legend(loc='best')\n",
    "        plt.title('Signal vs frequency: Lorentz function fit and points of interest '+ hashed, fontsize=20)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        global eval_df\n",
    "        \n",
    "        dfs_eval.append(evaluate_df)\n",
    "        \n",
    "        print('Eval appended.')\n",
    "        print('--------------------------------\\n')\n",
    "    eval_df = pd.concat(dfs_eval, ignore_index=True)\n",
    "    param_df = pd.DataFrame(\n",
    "        dfs_params, \n",
    "        columns=[\n",
    "            'minimized_freq_mean',\n",
    "            'minimized_freq_std',\n",
    "            'minimized_freq_std_err',\n",
    "            'freq_with_unc', \n",
    "            'hash'\n",
    "        ]\n",
    "    )\n",
    "    return eval_df, param_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4472842",
   "metadata": {},
   "source": [
    "# Running the model builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2459db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df, param_df = process_data(combined_df, log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ed388-e562-4a6a-9148-f60d688e3cee",
   "metadata": {},
   "source": [
    "### Creates a data frame with evaluated and parametrization data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a956b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(hashed):\n",
    "    group = hashed.split('-')[-3]\n",
    "    sensor = hashed.split('-')[-2]\n",
    "    time = hashed.split('-')[-1]\n",
    "    return pd.Series([group, sensor, time])\n",
    "\n",
    "eval_df[['group', 'sensor', 'time']] = eval_df['hash'].apply(extract_info)\n",
    "eval_df['time'] = pd.to_numeric(eval_df['time'])\n",
    "\n",
    "grouped_eval_df = eval_df.groupby(['hash']).min().sort_values(by=['group','sensor','time']).reset_index(drop=False)\n",
    "\n",
    "complete = pd.merge(addresses, grouped_eval_df[['x_eval','y_eval','hash']], on='hash', how='left')\n",
    "complete = complete.set_index(['hash']).drop('x_eval', axis=1)\n",
    "complete = complete.join(param_df.set_index('hash'))\n",
    "complete.to_csv(f'{folder}/complete_df.csv', sep=';')\n",
    "\n",
    "complete_dataset = f'{datetime.now()} - Complete dataset - EXPORTED \\n'\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(complete_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e494a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42a4b46-03b7-4cf9-9ee1-7a19d11b129c",
   "metadata": {},
   "source": [
    "### Applies a Lagergren model over minimized data in order to obtain a model for frequency decay on time.\n",
    "#### Provides a detailed report on fit params of the model.\n",
    "#### As result of this process, it is possilbe to estimate frequency shift of each time instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2305fb4-25aa-4dc1-9235-7e1f1606b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(t, f0, a, c):\n",
    "    return f0 * (1 - a * (1 - np.exp(-c * t)))\n",
    "\n",
    "def plot_data(x, y, title, xlabel, ylabel, legend):\n",
    "    plt.plot(x, y)\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend(legend)\n",
    "    plt.grid(True)\n",
    "\n",
    "sensors = complete['sensor'].unique()\n",
    "df_shift = pd.DataFrame(columns=['hash','shift', 'shift_value','shift_std'])\n",
    "for sensor in sensors:\n",
    "    g = complete['group'].loc[complete.sensor == sensor]\n",
    "    if g[0] == 'C':\n",
    "        label_group = ' - Control'\n",
    "    else:\n",
    "        label_group = ' - Test'\n",
    "        \n",
    "    df = complete.loc[(complete.group == g[0]) & (complete.sensor == sensor)]\n",
    "    df.plot(x='time',y='minimized_freq_mean',kind='scatter',yerr='minimized_freq_std',\n",
    "        figsize=(15,8),xlabel='Tempo [min]',ylabel = 'Média da frequência minimizada [MHz]',\n",
    "        title = 'Sensor ' + sensor + label_group)\n",
    "    plt.ticklabel_format(useOffset=False)\n",
    "    plt.show()\n",
    "    # Fit the function f(f0,a,c) = f0 * (1 - a * (1 - np.exp(-c * t)))\n",
    "    t = df.time \n",
    "    y = df.minimized_freq_mean\n",
    "    e = df.minimized_freq_std\n",
    "    popt, pcov = curve_fit(fit_model, t, y, absolute_sigma=True, maxfev=100000)\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "    f0 = popt[0]\n",
    "    a = popt[1]\n",
    "    c = popt[2]    \n",
    "    gmodel = Model(fit_model)\n",
    "    params = gmodel.make_params(f0=f0, a=a, c=c)\n",
    "    result = gmodel.fit(y, params, t=t)    \n",
    "    #Parameters with errors from LMFIT\n",
    "    f0uf = ufloat(result.params['f0'].value,result.params['f0'].stderr)\n",
    "    auf = ufloat(result.params['a'].value,result.params['a'].stderr)\n",
    "    cuf = ufloat(result.params['c'].value,result.params['c'].stderr)\n",
    "    shifts = []\n",
    "    shifts_values = []\n",
    "    shifts_std = []\n",
    "    for k in range(len(df)):\n",
    "        tk  = ufloat(df.time[k],df.minimized_freq_std[k])\n",
    "        ft = f0uf * (1 - auf * (1 - exp(-cuf * tk))) \n",
    "        shift = ft*1e6 - f0uf*1e6\n",
    "        shift_value = shift.nominal_value\n",
    "        shift_std = shift.std_dev\n",
    "        shifts.append(shift)\n",
    "        shifts_values.append(shift_value)\n",
    "        shifts_std.append(shift_std)\n",
    "    \n",
    "    df_aux           = pd.DataFrame(columns=['hash','shift', 'shift_value','shift_std'])\n",
    "    df_aux['hash']   = df.index\n",
    "    df_aux['shift']  = shifts\n",
    "    df_aux['shift_value']  = shifts_values\n",
    "    df_aux['shift_std']  = shifts_std\n",
    "    df_shift = pd.concat([df_shift, df_aux],ignore_index=True)\n",
    "    t30  = ufloat(df.time[-1],df.minimized_freq_std[-1])\n",
    "    f_t30 = f0uf * (1 - auf * (1 - exp(-cuf * t30)))\n",
    "    shift30 = f_t30*1e6 - f0uf*1e6\n",
    "    # Plot\n",
    "    plt.figure(figsize = (18,8))\n",
    "    ax = plt.axes()\n",
    "    ax.scatter(t, y, label='Raw data')\n",
    "    ax.errorbar(t, y, yerr=e,fmt=\"o\")\n",
    "    ax.plot(t,fit_model(t, *popt),'k',label=f'Fitted curve: f0={f0:.4f}, a={a:.4f}, c={c:.4f}')\n",
    "    ax.set_title(f'Lagergren - Sensor {sensor} {label_group}')\n",
    "    ax.set_ylabel('Minimized frequency mean [MHz]')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.legend()\n",
    "    ax.ticklabel_format(useOffset=False)\n",
    "    plt.legend()\n",
    "    plt.text(x=min(t),y=min(y),s=f'Frequence shift [Hz]:    {shift30:.8u}',horizontalalignment='left',verticalalignment='baseline') \n",
    "    plt.show()   \n",
    "    print(f'Summary - Sensor {sensor} {label_group}')\n",
    "    result_table2 = [\n",
    "        ['Parameter f0', f'{f0uf:.4u}'],\n",
    "        ['Parameter a', f'{auf:.4u}'],\n",
    "        ['Parameter c', f'{cuf:.4u}'],\n",
    "        ['Frequency t = 30 [MHz]', f'{f_t30:.4u}'],\n",
    "        ['Frequency shift [Hz]',f'{shift30:.8u}']\n",
    "    ]\n",
    "    print(str(tabulate(result_table2)))  \n",
    "    df_values = df[['group','sensor','time','y_eval','minimized_freq_mean','minimized_freq_std']]\n",
    "    headers = ['Group','Sensor','Time','Signal fit','Minimized frequency mean [MHz]','Std Dev']\n",
    "    tablefmt='psql'\n",
    "    values_table = tabulate(df_values,headers=headers,tablefmt=tablefmt)\n",
    "    print(values_table)\n",
    "    print('\\n')\n",
    "    print(result.fit_report())\n",
    "\n",
    "df_shift = df_shift.set_index(['hash'])\n",
    "complete_shifts = pd.merge(complete[['group','sensor','time']],df_shift,on='hash', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de8c28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Logging\n",
    "complete_shifts_export = f'\\n {str(datetime.now())} - Complete shifts list - EXPORTED \\n'\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(complete_shifts_export)\n",
    "    f.close()\n",
    "    \n",
    "complete_shifts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc27d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shift(data, hue, title):\n",
    "    plt.figure(figsize = (18,8))\n",
    "    sns.lineplot(\n",
    "        data=data, \n",
    "        x='time', \n",
    "        y='shift_value', \n",
    "        hue=hue\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Nominal shift f(t) - f(0) [MHz]')\n",
    "    plt.xlabel('Time [min]')\n",
    "    plt.show()\n",
    "\n",
    "plot_shift(complete_shifts, 'sensor', 'Frequency shift by sensor')\n",
    "plot_shift(complete_shifts, 'group', 'Frequency shift by group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,10))\n",
    "sns.boxplot(x='group', y='shift_value', data=complete_shifts, hue= 'group')\n",
    "sns.swarmplot(x='group', y='shift_value', data=complete_shifts, size=7, hue = 'time')\n",
    "plt.title('Frequency shift by group')\n",
    "plt.ylabel('Nominal shift f(t) - f(0) [MHz]')\n",
    "plt.xlabel('Group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0595f3f0-0250-46e5-9f5b-1ed445b0d55f",
   "metadata": {},
   "source": [
    "### Splits data to be used bootstrapped. This dataset is used on the classifier as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd8c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_shifts_clf = complete_shifts.drop(['sensor','shift'],axis=1).reset_index(drop=True)\n",
    "complete_shifts_clf = complete_shifts_clf[complete_shifts_clf.time != 0]\n",
    "complete_shifts_clf = complete_shifts_clf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ff0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_shifts_clf.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f47d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list_to_bootstrap = complete_shifts_clf[complete_shifts_clf['group'] == 'C'].values.tolist()\n",
    "c_list_to_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b29078",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list_to_bootstrap = complete_shifts_clf[complete_shifts_clf['group'] == 'T'].values.tolist()\n",
    "t_list_to_bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ac426-698f-4d0b-8538-45dbe51d1961",
   "metadata": {},
   "source": [
    "#### Bootstrapping by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e805198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n = 1000\n",
    "\n",
    "# Initialize a list to store the control data\n",
    "c_data = []\n",
    "t_data = []\n",
    "\n",
    "# Generate n random data points based on the minimized control data\n",
    "for _ in range(n):\n",
    "    c_original = random.choice(c_list_to_bootstrap)\n",
    "    c_data.append(c_original)\n",
    "    \n",
    "    t_original = random.choice(t_list_to_bootstrap)\n",
    "    t_data.append(t_original)\n",
    "\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(c_data)\n",
    "random.shuffle(t_data)\n",
    "\n",
    "complete_list = c_data + t_data\n",
    "random.shuffle(complete_list)\n",
    "\n",
    "# Example of the first 10 data points\n",
    "for i in range(5):\n",
    "    print(c_data[i])\n",
    "    print(t_data[i])\n",
    "    print(complete_list[i])\n",
    "    print('----------------------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63e0bd-e078-4c0a-bb0b-a9cc54aab6d7",
   "metadata": {},
   "source": [
    "### Classification model\n",
    "#### A set of different classifiers is applied to verify metrics and select best models to be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0050cd5-874a-48fa-9cbe-fbd29a7ebf80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bootstrapped = pd.DataFrame(complete_list, columns = ['group', 'time', 'frequency','standard deviation']) \n",
    "\n",
    "### Logging\n",
    "bootstrapped_shifts_export = f'\\n {str(datetime.now())} - Bootstrapped list - OK \\n'\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(bootstrapped_shifts_export)\n",
    "    f.close()\n",
    "\n",
    "bootstrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540f1e0-d655-4b11-83e1-cc4fae5fd7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize= (18,10))\n",
    "sns.violinplot(x='group', y='frequency', data=bootstrapped, hue= 'group', inner=\"quart\", ax=axs[0])\n",
    "sns.boxplot(x='group', y='frequency', data=bootstrapped, hue='time', notch=False, ax=axs[1])\n",
    "axs[0].set_xlabel(None)\n",
    "axs[1].set_xlabel(None)\n",
    "axs[0].set_ylabel(None)\n",
    "axs[1].set_ylabel(None)\n",
    "fig.suptitle('Frequency shift by group and time - Bootstrapped')\n",
    "fig.supxlabel('Group')\n",
    "fig.supylabel('Nominal shift f(t) - f(0) [MHz]')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (18,10))\n",
    "sns.violinplot(x='group', y='frequency', data=bootstrapped, hue= 'group', inner=\"quart\")\n",
    "sns.boxplot(x='group', y='frequency', data=bootstrapped, hue='time', notch=True)\n",
    "plt.title('Frequency shift by group by time - Bootstrapped')\n",
    "plt.ylabel('Nominal shift f(t) - f(0) [MHz]')\n",
    "plt.xlabel('Group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce61746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, name):\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    sns.heatmap(\n",
    "        conf_matrix, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues', \n",
    "        xticklabels=[\"C\", \"T\"], \n",
    "        yticklabels=[\"C\", \"T\"]\n",
    "    )\n",
    "    plt.title(f\"Confusion matrix - {name}\", fontsize=10)\n",
    "    plt.xlabel(\"Predicted\",fontsize=10)\n",
    "    plt.ylabel(\"True\",fontsize=10)\n",
    "    plt.show()\n",
    "\n",
    "# Define values to classifier\n",
    "X = np.array([(time, shift_value, shift_std) for _, time, shift_value, shift_std in complete_list])\n",
    "y = np.array([group for group, _, _, _ in complete_list])\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "classifiers = {\n",
    "    \"Logistics Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"SVM (SVC)\": SVC(kernel='linear'),\n",
    "    \"SVM (RBF)\": SVC(kernel='rbf'),\n",
    "    \"SVM (Poly)\": SVC(kernel='poly')\n",
    "}\n",
    "\n",
    "# Metrics\n",
    "metrics_table = []\n",
    "\n",
    "# Run classifier\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # metrics calculate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # cross val\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Add the metrics to the table\n",
    "    metrics_table.append({\n",
    "        'Algorithm': name,\n",
    "        'CV Avg Accuracy': scores.mean(),\n",
    "        'CV Acc std dev': scores.std(),\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': report['weighted avg']['precision'],\n",
    "        'Recall': report['weighted avg']['recall'],\n",
    "        'F1-Score': report['weighted avg']['f1-score'],\n",
    "        'Support': report['weighted avg']['support'],\n",
    "    })\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    plot_confusion_matrix(conf_matrix, name)\n",
    "    \n",
    "    # print cross val metrics\n",
    "    print(f\"Classifier: {name}\")\n",
    "    print(f\"Accuracy avg: {scores.mean()}\")\n",
    "    print(f\"Accuracy 5 runs: {scores}\")\n",
    "    print(\"\\n\")\n",
    "    print('-----------------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "    metrics_table_export = [\n",
    "        ['Classifier', name],\n",
    "        ['Accuracy avg', scores.mean()]\n",
    "    ]\n",
    "    ### Logging\n",
    "    metrics_list_export = f'\\n {str(datetime.now())} - Metrics list - EXPORTED \\n'\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(metrics_list_export)\n",
    "        f.write(str(tabulate(metrics_table_export)))\n",
    "\n",
    "# Convert the table into a Pandas DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = metrics_df.sort_values(by='CV Avg Accuracy', ascending=False)\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfebb18-5ad1-415f-b9bd-ab4ce58e7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logging\n",
    "all_metrics_df = tabulate(metrics_df, headers='keys', tablefmt='psql')\n",
    "\n",
    "all_metrics = f'{str(datetime.now())} - All metrics \\n' + str(all_metrics_df) + '\\n'\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(all_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd45bfe-33cb-4106-a2fa-e20da7ec5352",
   "metadata": {},
   "source": [
    "#### Model export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d5473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "model_filename = f\"{folder}/model_classifier.pkl\"\n",
    "joblib.dump(clf, model_filename)\n",
    "\n",
    "# Log the export\n",
    "model_file_export = f\"\\n{datetime.now()} - Model file exported. Analysis is complete!\"\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(model_file_export)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
