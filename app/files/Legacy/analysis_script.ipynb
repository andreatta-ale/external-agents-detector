{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7238e0ec",
   "metadata": {},
   "source": [
    "# Data import and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d78d123-43d5-4133-90de-1cf0fd6d29ec",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e355769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import shutil\n",
    "import warnings\n",
    "\n",
    "from datetime import date, datetime\n",
    "from tabulate import tabulate\n",
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "\n",
    "from lmfit import Minimizer, Parameters, report_fit, minimize\n",
    "from lmfit.models import SplitLorentzianModel, LinearModel\n",
    "from lmfit import Model\n",
    "\n",
    "from scipy.signal import argrelmax,hilbert, find_peaks, peak_widths\n",
    "from scipy import optimize\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from uncertainties import ufloat,unumpy\n",
    "from uncertainties.umath import *\n",
    "\n",
    "pd.set_option('display.max_rows', 50)\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb26487-b1f1-40a2-a94f-ca76fed6a543",
   "metadata": {},
   "source": [
    "### Selecting data folder\n",
    "#### Creates result folder for output saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11340b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Tk()\n",
    "root.withdraw()\n",
    "folder_path = filedialog.askdirectory()\n",
    "print('Folder path: ',folder_path)\n",
    "\n",
    "file_path = folder_path + '/**/*.csv'\n",
    "result_folder = '/res'\n",
    "folder = folder_path + result_folder\n",
    "if os.path.isdir(folder):\n",
    "    print('Result folder already exists, cleaning...\\n (If an error occurs, consider remove Result folder manually.)')\n",
    "    try:\n",
    "        shutil.rmtree(folder)\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s : %s\" % (folder, e.strerror))\n",
    "    os.mkdir(folder)\n",
    "    print ('Result folder created.')\n",
    "else:\n",
    "    os.mkdir(folder)\n",
    "    print ('Result folder created.')\n",
    "    \n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "print('Result folder path: ',folder)\n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "\n",
    "### Logging\n",
    "log_file = folder + '/log.txt'\n",
    "\n",
    "lf = open(log_file, 'w')\n",
    "\n",
    "with open(log_file, 'w') as f:\n",
    "    f.write(str(datetime.now()) + ' - ' + folder)\n",
    "\n",
    "lf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f4bd1a-a1e9-4e19-b3e2-606fad3d5353",
   "metadata": {},
   "source": [
    "### Acquiring individual files addresses \n",
    "#### Splits into group, sensor and time, adding a hash column used for filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47facbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = pd.DataFrame(glob.glob(file_path,recursive = True),columns = ['Path'])\n",
    "address = []\n",
    "\n",
    "for index, row in files.iterrows():\n",
    "    data = []\n",
    "    data.append(row['Path'])\n",
    "    address.append(data)\n",
    "\n",
    "addresses = pd.DataFrame(\n",
    "    address, \n",
    "    columns = ['address'])\n",
    "\n",
    "print('Main data frame shape: ', addresses.shape)\n",
    "\n",
    "#for item in addresses.address:\n",
    "#    print(item)\n",
    "    \n",
    "group_name = []\n",
    "sensor_name = []\n",
    "time_name = []\n",
    "hash_name = []\n",
    "\n",
    "for path in addresses['address']:\n",
    "    group_name_lst = []\n",
    "    sensor_name_lst = []\n",
    "    time_name_lst = []\n",
    "    hash_name_lst = []\n",
    "    \n",
    "    group_name_lst.append(path.split('/')[-1].split('\\\\')[-3])\n",
    "    sensor_name_lst.append(path.split('/')[-1].split('\\\\')[-2])\n",
    "    \n",
    "    k = path.rfind('\\\\')\n",
    "    time_name_lst.append(int(path[k + 1:].replace('.CSV', '')))\n",
    "    hash_name_lst.append(\n",
    "        path.split('/')[-1].split('\\\\')[-3] + \\\n",
    "        '-' + \\\n",
    "        str(path.split('/')[-1].split('\\\\')[-2]) + \\\n",
    "        '-' + \\\n",
    "        str(path[k + 1:].replace('.CSV', ''))\n",
    "    )\n",
    "    \n",
    "    time_name.append(time_name_lst)\n",
    "    sensor_name.append(sensor_name_lst)\n",
    "    group_name.append(group_name_lst)\n",
    "    hash_name.append(hash_name_lst)\n",
    "\n",
    "group = pd.DataFrame(group_name, columns=['group'])\n",
    "sensor = pd.DataFrame(sensor_name, columns=['sensor'])\n",
    "time = pd.DataFrame(time_name, columns=['time'])\n",
    "hashed = pd.DataFrame(hash_name, columns=['hash'])\n",
    "\n",
    "addresses['group'] = group\n",
    "addresses['sensor'] = sensor\n",
    "addresses['time'] = time\n",
    "addresses['hash'] = hashed\n",
    "\n",
    "#addresses['sensor'] = pd.to_numeric(addresses['sensor'])\n",
    "\n",
    "addresses = addresses.sort_values(\n",
    "    by=['group','sensor','time','address'], \n",
    "    ignore_index=True, \n",
    "    ascending=True\n",
    ")\n",
    "\n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "\n",
    "text = '''\n",
    "Analysis\n",
    "'''\n",
    "\n",
    "#text = addresses['Sensor'].unique()\n",
    "\n",
    "print(text + '\\n')\n",
    "\n",
    "print('---------------------------------------------------------------------------\\n')\n",
    "\n",
    "addresses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb654d-5b6e-4373-bfab-2508db4cee88",
   "metadata": {},
   "source": [
    "#### Provides intel on quantity of files to be scanned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = len(addresses.index)\n",
    "print('Space: ', space)\n",
    "fst_counter = 0\n",
    "today = date.today()\n",
    "\n",
    "### Logging\n",
    "\n",
    "files_address_list = f'{str(datetime.now())} - Address list - OK \\n'\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(files_address_list)\n",
    "    f.close()\n",
    "\n",
    "addresses = addresses.reset_index(drop=True).set_index('address')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeff516-a409-4ff3-b541-5d872e5e2147",
   "metadata": {},
   "source": [
    "### Scans the addresses data frame, reads data of each file and assembles them into a combined data frame, that contains frequency and signal data\n",
    "### Also, rescales frequency values to signal's order of magnitude (multiplied by $1 \\times 10^{-6}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103861ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for path in addresses.index:\n",
    "    global combined_df\n",
    "    fst_counter += 1\n",
    "    param = path\n",
    "    \n",
    "    group = addresses.loc[path,'group']\n",
    "    sensor = addresses.loc[path,'sensor']\n",
    "    time = addresses.loc[path,'time']\n",
    "    hashed = addresses.loc[path,'hash']\n",
    " \n",
    "    df_import = pd.DataFrame(pd.read_csv(param,skiprows = range(0, 2)))\n",
    "    df_import.drop(df_import.columns[2], axis = 1, inplace = True)\n",
    "    df_import.columns.values[0] = 'frequency'\n",
    "    df_import.columns.values[1] = 'signal'\n",
    "    df_import['group'] = group\n",
    "    df_import['sensor'] = sensor                                          \n",
    "    df_import['time'] = time\n",
    "    df_import['hash'] = hashed\n",
    "    \n",
    "    dfs.append(df_import)\n",
    "\n",
    "combined_df = pd.concat(dfs,ignore_index=True)\n",
    "\n",
    "combined_dataset = f'{str(datetime.now())} - Combined dataset - OK \\n'\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(combined_dataset)\n",
    "    f.close()\n",
    "combined_df['frequency'] = combined_df['frequency'] * 1.e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bb8461",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a3dbaa-91d8-4106-8a31-eb9fc869a787",
   "metadata": {},
   "source": [
    "### Exports combined and addresses data frames to results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5397b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_export = f'{str(datetime.now())} - Combined and address datasets - EXPORTED \\n'\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(datasets_export)\n",
    "    f.close()\n",
    "    \n",
    "combined_df.to_csv(folder + '/combined_df.csv', sep=';', index=False)\n",
    "addresses.to_csv(folder + '/addresses.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d7ffe7",
   "metadata": {},
   "source": [
    "# Defining functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0bd80",
   "metadata": {},
   "source": [
    "### Split Lorentzian funcion\n",
    "#### Function used to fit data to model\n",
    "\n",
    "$$\n",
    "f(x; A, \\mu, \\sigma, \\sigma_r) = \\frac{2 A}{\\pi (\\sigma+\\sigma_r)} \\big[\\frac{\\sigma^2}{(x - \\mu)^2 + \\sigma^2} * H(\\mu-x) + \\frac{\\sigma_r^2}{(x - \\mu)^2 + \\sigma_r^2} * H(x-\\mu)\\big] + (m x + b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a28980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Lorentzian function\n",
    "def SplitLorentzianFunc(x):\n",
    "    global aux\n",
    "    A = ((2 * aux[0]) / (np.pi * (aux[2] + aux[3])))\n",
    "    B = ((aux[2] ** 2) / (((x - aux[1]) ** 2) + aux[2] ** 2))\n",
    "    C = ((aux[3] ** 2) / (((x - aux[1]) ** 2) + aux[3] ** 2))\n",
    "    D = np.heaviside(aux[1] - x, 0)\n",
    "    E = np.heaviside(x - aux[1], 0)\n",
    "    F = (aux[4] * x + aux[5])\n",
    "\n",
    "    return (A * ((B*D) + (C*E))) + F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66647f34",
   "metadata": {},
   "source": [
    "# Slicing data for analysis\n",
    "\n",
    "### Applies fitting model (Lorentzian) to each experimental dataset, retrieving a evaluation dataset based on the model. After fitting, applies an optimization function over a Monte Carlo Simulation to find the interesting point: the minimized frequency used to evaluate delocation on passing time of experiment\n",
    "\n",
    "\n",
    "#### For each instance of time, plots original data, accuracy rate between Monte Carlo Simulation average frequency and minimized average frequency, and fitted curve over original data, pinpointing those values alongside minimal point from original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2459db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hash = combined_df.hash.unique()\n",
    "dfs_params = []\n",
    "dfs_eval = []\n",
    "frequency_shift = []\n",
    "\n",
    "for i in unique_hash:\n",
    "    \n",
    "    hashed = i\n",
    "    xy0 = combined_df[['frequency','signal']].loc[combined_df['hash'] == hashed]\n",
    "    xy0 = xy0.reset_index(drop=True)\n",
    "\n",
    "    interval = 13\n",
    "    min_value_idx = xy0.loc[xy0['signal'] == xy0['signal'].min()].index[0]\n",
    "    idx = range(min_value_idx - interval, min_value_idx + interval)\n",
    "\n",
    "    x = np.array(xy0['frequency'])[idx]\n",
    "    y = np.array(xy0['signal'])[idx]\n",
    "    \n",
    "    # Model parametrization\n",
    "    peak = SplitLorentzianModel() #prefix='slm_')\n",
    "    linemod1 = LinearModel() #prefix='lm1_')\n",
    "    #linemod2 = LinearModel(prefix='lm2_')\n",
    "    pars = Parameters()\n",
    "    pars += peak.guess(y, x=x)\n",
    "    pars += linemod1.make_params(intercept=y.min(), slope=0)\n",
    "    #pars += linemod2.make_params(intercept=y.min(), slope=0)\n",
    "    mod = linemod1 + peak # + linemod2\n",
    "\n",
    "    # Fit model\n",
    "    result = mod.fit(y, pars, x=x)\n",
    "    \n",
    "    # Printing report and related information\n",
    "    print(result.fit_report(min_correl=0.25))\n",
    "\n",
    "    ### Logging\n",
    "\n",
    "    lmfit_report = f'\\n{str(datetime.now())} - LMFIT result report {i} \\n' + str(result.fit_report(min_correl=0.25))\n",
    "\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(lmfit_report)\n",
    "        f.close()\n",
    "    \n",
    "    # Determining the point to be evaluated on the frequency shift   \n",
    "    x_eval = np.linspace(min(x), max(x), num = 10000)\n",
    "    y_eval = result.eval(result.params, x=x_eval)\n",
    "\n",
    "    evaluate_df = pd.DataFrame(x_eval, columns=['x_eval'])\n",
    "    evaluate_df['y_eval'] = y_eval\n",
    "    evaluate_df['hash'] = hashed\n",
    "    \n",
    "    plot_x = evaluate_df['x_eval'].loc[evaluate_df['y_eval'] == evaluate_df['y_eval'].min()]\n",
    "    plot_y = evaluate_df['y_eval'].loc[evaluate_df['y_eval'] == evaluate_df['y_eval'].min()]\n",
    "\n",
    "    # Min point\n",
    "    minimized_freq = []\n",
    "\n",
    "    n = 1000\n",
    "    \n",
    "    for j in range(n):\n",
    "        aux = np.array([\n",
    "            np.random.normal( # 0\n",
    "                loc=result.params['amplitude'].value, \n",
    "                scale=result.params['amplitude'].stderr\n",
    "            ),\n",
    "            np.random.normal( # 1\n",
    "                loc=result.params['center'].value, \n",
    "                scale=result.params['center'].stderr\n",
    "            ), \n",
    "            np.random.normal( # 2\n",
    "                loc=result.params['sigma'].value, \n",
    "                scale=result.params['sigma'].stderr\n",
    "            ),\n",
    "            np.random.normal( # 3\n",
    "                loc=result.params['sigma_r'].value, \n",
    "                scale=result.params['sigma_r'].stderr\n",
    "            ),\n",
    "            np.random.normal( # 4\n",
    "                loc=result.params['slope'].value, \n",
    "                scale=result.params['slope'].stderr\n",
    "            ),\n",
    "            np.random.normal( # 5\n",
    "                loc=result.params['intercept'].value, \n",
    "                scale=result.params['intercept'].stderr\n",
    "                )\n",
    "            ])\n",
    "\n",
    "        find_fmin = optimize.fmin(\n",
    "            SplitLorentzianFunc,\n",
    "            xy0['frequency'][min_value_idx], \n",
    "            full_output=True,\n",
    "            disp=0\n",
    "        )\n",
    "\n",
    "        find_fmin_point = np.array([find_fmin[0].item(), find_fmin[1]])\n",
    "\n",
    "        minimized_freq.append(find_fmin[0])\n",
    "\n",
    "        j = j + 1\n",
    "\n",
    "    minimized_freq = np.concatenate(minimized_freq).ravel()\n",
    "    minimized_freq = minimized_freq[(minimized_freq > 0.43) & (minimized_freq < 0.444)]\n",
    "\n",
    "    minimized_freq_mean = np.array(minimized_freq).mean()\n",
    "    minimized_freq_std = np.array(minimized_freq).std()\n",
    "    minimized_freq_std_err = minimized_freq_std / np.sqrt(n)\n",
    "    freq = ufloat(minimized_freq_mean,minimized_freq_std)*1e6\n",
    "    pfloat = minimized_freq_mean/plot_x.values.item()\n",
    "    \n",
    "    values_lst = [\n",
    "        minimized_freq_mean, \n",
    "        minimized_freq_std, \n",
    "        minimized_freq_std_err,\n",
    "        freq,\n",
    "        hashed\n",
    "    ]\n",
    "\n",
    "    dfs_params.append(values_lst)\n",
    "\n",
    "    print('\\n')\n",
    "    print('----- Results -----')\n",
    "    result_table = [\n",
    "        ['Optimized frequency mean (fmin)', minimized_freq_mean],\n",
    "        ['Optimized frequency standard deviation (fmin)', minimized_freq_std],\n",
    "        ['Optimized frequency standard error (fmin)', minimized_freq_std_err],\n",
    "        ['Optimized frequency mean with uncertainties', freq],\n",
    "        ['Accuracy of estimated frequency mean / SMC',pfloat]\n",
    "    ]\n",
    "    \n",
    "    print(str(tabulate(result_table)))\n",
    "    print('\\n')\n",
    "\n",
    "    ### Logging\n",
    "\n",
    "    variables_report = f'{str(datetime.now())} - Results {i} \\n' + str(tabulate(result_table)) + '\\n'\n",
    "\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(variables_report)\n",
    "        f.close()\n",
    "\n",
    "    \n",
    "    # PLOTING\n",
    "\n",
    "    # Primary data\n",
    "    plt.figure()\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    ax = xy0.plot(\n",
    "        x = 'frequency', \n",
    "        y = 'signal', \n",
    "        kind='scatter',\n",
    "        figsize = (16,4), \n",
    "        grid=True, \n",
    "        legend=True\n",
    "    )\n",
    "    ax.set_title(label = 'Initial data ' + hashed, pad=20, fontdict={'fontsize':20})\n",
    "    ax.set_xlabel('Frequency [MHz]')\n",
    "    ax.set_ylabel('Signal')\n",
    "    plt.show()\n",
    "    print('\\n')\n",
    "    \n",
    "    #Accuracy between minimized frequency mean and MCS frequency mean\n",
    "    xfloat = np.linspace(0.98, 1.02, num = 100)\n",
    "    yfloat = np.linspace(0, 0, num = 100)\n",
    "    fig = plt.figure(figsize = (16,4))\n",
    "    plt.plot(xfloat,yfloat)\n",
    "    plt.plot(pfloat,0,color='k',marker='|', markersize = 15, label='Optimized frequency mean LMFIT / Optimized frequency mean SMC')\n",
    "    plt.text(\n",
    "        x=pfloat, \n",
    "        y=0.02, \n",
    "        s='Accuracy:    {:.8}'.format(pfloat), \n",
    "        horizontalalignment='right',\n",
    "        verticalalignment='baseline'\n",
    "    ) \n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Accuracy between frequency mean LMFIT and SMC optimized frequency mean', fontsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "    # Fit model data plot    \n",
    "    fig = plt.figure(figsize = (16,8))\n",
    "    plt.plot(x, y, 'o')\n",
    "    plt.plot(x_eval, y_eval, 'r-', label='Best fit')\n",
    "\n",
    "    plt.plot(\n",
    "        xy0['frequency'][min_value_idx],\n",
    "        xy0['signal'][min_value_idx],\n",
    "        marker = 'D',\n",
    "        color='orange', \n",
    "        markersize=8,\n",
    "        label='Original data minimum frequency'\n",
    "    )\n",
    "    \n",
    "    labels = evaluate_df['hash'].loc[evaluate_df['y_eval'] == evaluate_df['y_eval'].min()]\n",
    "    \n",
    "    plt.plot(\n",
    "        plot_x, \n",
    "        plot_y, \n",
    "        label='Lorentz minimum frequency (LMFIT)', \n",
    "        color='green', \n",
    "        marker='s', \n",
    "        markersize=8\n",
    "    )\n",
    "    plt.plot(\n",
    "        minimized_freq_mean, \n",
    "        plot_y, \n",
    "        label='SMC average frequency', \n",
    "        color='k', \n",
    "        marker='o', \n",
    "        markersize=8\n",
    "    )\n",
    "    plt.xlabel('Frequency [MHz]')\n",
    "    plt.ylabel('Signal')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Signal vs frequency: Lorentz function fit and points of interest '+ hashed, fontsize=20)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\n')\n",
    "        \n",
    "    global eval_df\n",
    "    \n",
    "    dfs_eval.append(evaluate_df)\n",
    "    \n",
    "    print('Eval appended.')\n",
    "    print('--------------------------------\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8ed388-e562-4a6a-9148-f60d688e3cee",
   "metadata": {},
   "source": [
    "### Creates a data frame with evaluated and parametrization data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a956b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.concat(\n",
    "    dfs_eval,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "eval_df['y_eval'] = eval_df['y_eval']\n",
    "\n",
    "param_df = pd.DataFrame(\n",
    "    dfs_params, \n",
    "    columns=[\n",
    "        'minimized_freq_mean',\n",
    "        'minimized_freq_std',\n",
    "        'minimized_freq_std_err',\n",
    "        'freq_with_unc', \n",
    "        'hash'\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_group = []\n",
    "eval_sensor = []\n",
    "eval_time = []\n",
    "\n",
    "for hashed in eval_df.hash:\n",
    "    eval_group_lst = []\n",
    "    eval_sensor_lst = []\n",
    "    eval_time_lst = []\n",
    "    \n",
    "    eval_group_lst.append(hashed.split('-')[-3])\n",
    "    eval_sensor_lst.append(hashed.split('-')[-2])\n",
    "    eval_time_lst.append(hashed.split('-')[-1])\n",
    "    \n",
    "    eval_group.append(eval_group_lst)\n",
    "    eval_sensor.append(eval_sensor_lst)\n",
    "    eval_time.append(eval_time_lst)\n",
    "\n",
    "group_df = pd.DataFrame(eval_group, columns=['group'])\n",
    "sensor_df = pd.DataFrame(eval_sensor, columns=['sensor'])\n",
    "time_df = pd.DataFrame(eval_time, columns=['time'])\n",
    "\n",
    "eval_df['group'] = group_df\n",
    "eval_df['sensor'] = sensor_df\n",
    "eval_df['time'] = time_df\n",
    "eval_df['time'] = pd.to_numeric(eval_df['time'])\n",
    "grouped_eval_df = eval_df.groupby(\n",
    "    ['hash']).min().sort_values(\n",
    "    by=['group','sensor','time']\n",
    ").reset_index(drop=False)\n",
    "\n",
    "complete = pd.merge(addresses,grouped_eval_df[['x_eval','y_eval','hash']],on='hash', how='left')\n",
    "complete = complete.set_index(['hash']).drop('x_eval', axis=1)\n",
    "complete = complete.join(param_df.set_index('hash'))\n",
    "complete.to_csv(folder + '/complete_df.csv', sep=';')\n",
    "\n",
    "complete_dataset = f'{str(datetime.now())} - Complete dataset - EXPORTED \\n'\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(complete_dataset)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e494a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42a4b46-03b7-4cf9-9ee1-7a19d11b129c",
   "metadata": {},
   "source": [
    "### Applies a Lagergren model over minimized data in order to obtain a model for frequency decay on time.\n",
    "#### Provides a detailed report on fit params of the model.\n",
    "#### As result of this process, it is possilbe to estimate frequency shift of each time instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a2cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(t, f0, a, c):\n",
    "    return f0 * (1 - a * (1 - np.exp(-c * t)))\n",
    "\n",
    "sensors = complete['sensor'].unique()\n",
    "\n",
    "df_shift = pd.DataFrame(columns=['hash','shift', 'shift_value','shift_std'])\n",
    "\n",
    "for sensor in sensors:\n",
    "    g = complete['group'].loc[complete.sensor == sensor]\n",
    "    if g[0] == 'C':\n",
    "        label_group = ' - Control'\n",
    "    else:\n",
    "        label_group = ' - Test'\n",
    "        \n",
    "    df = complete.loc[(complete.group == g[0]) & (complete.sensor == sensor)]\n",
    "        \n",
    "    df.plot(\n",
    "        x='time',\n",
    "        y='minimized_freq_mean',\n",
    "        kind='scatter',\n",
    "        yerr='minimized_freq_std',\n",
    "        figsize=(15,8),\n",
    "        xlabel='Tempo [min]',\n",
    "        ylabel = 'Média da frequência minimizada [MHz]',\n",
    "        title = 'Sensor ' + sensor + label_group\n",
    "    )\n",
    "    plt.ticklabel_format(useOffset=False)\n",
    "    plt.show()\n",
    "    \n",
    "    # Fit the function f(f0,a,c) = f0 * (1 - a * (1 - np.exp(-c * t)))\n",
    "    t = df.time \n",
    "    y = df.minimized_freq_mean\n",
    "    e = df.minimized_freq_std\n",
    "        \n",
    "    popt, pcov = curve_fit(fit_model, t, y, absolute_sigma=True, maxfev=100000)\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "\n",
    "    f0 = popt[0]\n",
    "    a = popt[1]\n",
    "    c = popt[2]\n",
    "\n",
    "    \n",
    "    gmodel = Model(fit_model)\n",
    "    params = gmodel.make_params(f0=f0, a=a, c=c)\n",
    "    result = gmodel.fit(y, params, t=t)    \n",
    "    \n",
    "    #Parameters with errors from LMFIT\n",
    "    f0uf = ufloat(result.params['f0'].value,result.params['f0'].stderr)\n",
    "    auf = ufloat(result.params['a'].value,result.params['a'].stderr)\n",
    "    cuf = ufloat(result.params['c'].value,result.params['c'].stderr)\n",
    "    \n",
    "    \n",
    "    shifts = []\n",
    "    shifts_values = []\n",
    "    shifts_std = []\n",
    "    \n",
    "    for k in range(len(df)):\n",
    "        tk  = ufloat(df.time[k],df.minimized_freq_std[k])\n",
    "        ft = f0uf * (1 - auf * (1 - exp(-cuf * tk))) \n",
    "        shift = ft*1e6 - f0uf*1e6\n",
    "        shift_value = shift.nominal_value\n",
    "        shift_std = shift.std_dev\n",
    "        shifts.append(shift)\n",
    "        shifts_values.append(shift_value)\n",
    "        shifts_std.append(shift_std)\n",
    "    \n",
    "    df_aux           = pd.DataFrame(columns=['hash','shift', 'shift_value','shift_std'])\n",
    "    df_aux['hash']   = df.index\n",
    "    df_aux['shift']  = shifts\n",
    "    df_aux['shift_value']  = shifts_values\n",
    "    df_aux['shift_std']  = shifts_std\n",
    "    \n",
    "    df_shift = pd.concat([df_shift, df_aux],ignore_index=True)\n",
    "    \n",
    "    t30  = ufloat(df.time[-1],df.minimized_freq_std[-1])\n",
    "    f_t30 = f0uf * (1 - auf * (1 - exp(-cuf * t30)))\n",
    "\n",
    "    shift30 = f_t30*1e6 - f0uf*1e6\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize = (18,8))\n",
    "    ax = plt.axes()\n",
    "    ax.scatter(t, y, label='Raw data')\n",
    "    ax.errorbar(t, y, yerr=e,fmt=\"o\")\n",
    "    ax.plot(\n",
    "        t, \n",
    "        fit_model(t, *popt), \n",
    "        'k',\n",
    "        label='Fitted curve: f0=%5.4f, a=%5.4f, c=%5.4f' % tuple(popt)\n",
    "    )\n",
    "    ax.set_title(r'Lagergren - Sensor ' + sensor + label_group)\n",
    "    ax.set_ylabel('Minimized frequency mean [MHz]')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.legend()\n",
    "    ax.ticklabel_format(useOffset=False)\n",
    "    plt.legend()\n",
    "    plt.text(\n",
    "        x=min(t), \n",
    "        y=min(y), \n",
    "        s='Frequence shift [Hz]:    {:.8u}'.format(shift30), \n",
    "        horizontalalignment='left',\n",
    "        verticalalignment='baseline'\n",
    "    ) \n",
    "    plt.show()   \n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------------------')\n",
    "    print('Summary - Sensor ' + sensor + label_group)\n",
    "\n",
    "    result_table2 = [\n",
    "        ['Parameter f0', '{:.4u}'.format(f0uf)],\n",
    "        ['Parameter a', '{:.4u}'.format(auf)],\n",
    "        ['Parameter c', '{:.4u}'.format(cuf)],\n",
    "        ['Frequency t = 30 [MHz]', '{:.4u}'.format(f_t30)],\n",
    "        ['Frequency shift [Hz]','{:.8u}'.format(shift30)]\n",
    "    ]\n",
    "    \n",
    "    print(str(tabulate(result_table2)))\n",
    "    print('\\n')\n",
    "    \n",
    "    df_values = df[['group','sensor','time','y_eval','minimized_freq_mean','minimized_freq_std']]\n",
    "    headers = ['Group','Sensor','Time','Signal fit','Minimized frequency mean [MHz]','Std Dev']\n",
    "    tablefmt='psql'\n",
    "\n",
    "    values_table = tabulate(df_values,headers=headers,tablefmt=tablefmt)\n",
    "    \n",
    "    print(values_table)\n",
    "    print('\\n')\n",
    "    print(result.fit_report())\n",
    "    print('------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "    ### Logging\n",
    "\n",
    "    result_report2 = f'{str(datetime.now())} - Summary - Sensor {sensor} {label_group} \\n' + str(tabulate(result_table2)) + '\\n'\n",
    "    result_report3 = f'{str(datetime.now())} - Values {sensor} {label_group} \\n' + str(values_table) + '\\n'\n",
    "    result_report4 = f'{str(datetime.now())} - Values {sensor} {label_group} \\n' + str(result.fit_report()) + '\\n'\n",
    "    \n",
    "\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(result_report2)\n",
    "        f.write(result_report3)\n",
    "        f.write(result_report4)\n",
    "        f.close()\n",
    "\n",
    "df_shift = df_shift.set_index(['hash'])\n",
    "complete_shifts = pd.merge(complete[['group','sensor','time']],df_shift,on='hash', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de8c28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Logging\n",
    "complete_shifts_export = f'\\n {str(datetime.now())} - Complete shifts list - EXPORTED \\n'\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(complete_shifts_export)\n",
    "    f.close()\n",
    "    \n",
    "complete_shifts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc27d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,8))\n",
    "sns.lineplot(\n",
    "    data=complete_shifts, \n",
    "    x='time', \n",
    "    y='shift_value', \n",
    "    hue='sensor'\n",
    ")\n",
    "plt.title('Frequency shift by sensor')\n",
    "plt.ylabel('Nominal shift f(t) - f(0) [MHz]')\n",
    "plt.xlabel('Time [min]')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (18,8))\n",
    "sns.lineplot(\n",
    "    data=complete_shifts, \n",
    "    x='time', \n",
    "    y='shift_value', \n",
    "    hue='group'\n",
    ")\n",
    "plt.title('Frequency shift by group')\n",
    "plt.ylabel('Nominal shift f(t) - f(0) [MHz]')\n",
    "plt.xlabel('Time [min]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,10))\n",
    "sns.boxplot(x='group', y='shift_value', data=complete_shifts, hue= 'group')\n",
    "sns.swarmplot(x='group', y='shift_value', data=complete_shifts, size=7, hue = 'time')\n",
    "plt.title('Frequency shift by group')\n",
    "plt.ylabel('Nominal shift f(t) - f(0) [MHz]')\n",
    "plt.xlabel('Group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0595f3f0-0250-46e5-9f5b-1ed445b0d55f",
   "metadata": {},
   "source": [
    "### Splits data to be used bootstrapped. This dataset is used on the classifier as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd8c0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_shifts_clf = complete_shifts.drop(['sensor','shift'],axis=1).reset_index(drop=True)\n",
    "complete_shifts_clf = complete_shifts_clf[complete_shifts_clf.time != 0]\n",
    "complete_shifts_clf = complete_shifts_clf.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ff0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_shifts_clf.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f47d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list_to_bootstrap = complete_shifts_clf[complete_shifts_clf['group'] == 'C'].values.tolist()\n",
    "c_list_to_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b29078",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list_to_bootstrap = complete_shifts_clf[complete_shifts_clf['group'] == 'T'].values.tolist()\n",
    "t_list_to_bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ac426-698f-4d0b-8538-45dbe51d1961",
   "metadata": {},
   "source": [
    "#### Bootstrapping by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e805198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n = 1000\n",
    "\n",
    "# Inicializar uma lista para armazenar os dados de controle\n",
    "c_data = []\n",
    "t_data = []\n",
    "\n",
    "# Gerar n pontos de dados aleatórios com base nos dados de controle minimizados\n",
    "for _ in range(n):\n",
    "    c_original = random.choice(c_list_to_bootstrap)\n",
    "    c_data.append(c_original)\n",
    "    \n",
    "    t_original = random.choice(t_list_to_bootstrap)\n",
    "    t_data.append(t_original)\n",
    "\n",
    "\n",
    "# Embaralhar os dados\n",
    "random.shuffle(c_data)\n",
    "random.shuffle(t_data)\n",
    "\n",
    "complete_list = c_data + t_data\n",
    "random.shuffle(complete_list)\n",
    "\n",
    "# Exemplo dos primeiros 10 pontos de dados\n",
    "for i in range(5):\n",
    "    print(c_data[i])\n",
    "    print(t_data[i])\n",
    "    print(complete_list[i])\n",
    "    print('----------------------\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63e0bd-e078-4c0a-bb0b-a9cc54aab6d7",
   "metadata": {},
   "source": [
    "### Classification model\n",
    "#### A set of different classifiers is applied to verify metrics and select best models to be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0050cd5-874a-48fa-9cbe-fbd29a7ebf80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bootstrapped = pd.DataFrame(complete_list, columns = ['group', 'time', 'frequency','standard deviation']) \n",
    "\n",
    "### Logging\n",
    "bootstrapped_shifts_export = f'\\n {str(datetime.now())} - Bootstrapped list - OK \\n'\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(bootstrapped_shifts_export)\n",
    "    f.close()\n",
    "\n",
    "bootstrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e540f1e0-d655-4b11-83e1-cc4fae5fd7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize= (18,10))\n",
    "sns.violinplot(x='group', y='frequency', data=bootstrapped, hue= 'group', inner=\"quart\", ax=axs[0])\n",
    "sns.boxplot(x='group', y='frequency', data=bootstrapped, hue='time', notch=False, ax=axs[1])\n",
    "axs[0].set_xlabel(None)\n",
    "axs[1].set_xlabel(None)\n",
    "axs[0].set_ylabel(None)\n",
    "axs[1].set_ylabel(None)\n",
    "fig.suptitle('Frequency shift by group and time - Bootstrapped')\n",
    "fig.supxlabel('Group')\n",
    "fig.supylabel('Nominal shift f(t) - f(0) [MHz]')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (18,10))\n",
    "sns.violinplot(x='group', y='frequency', data=bootstrapped, hue= 'group', inner=\"quart\")\n",
    "sns.boxplot(x='group', y='frequency', data=bootstrapped, hue='time', notch=True)\n",
    "plt.title('Frequency shift by group by time - Bootstrapped')\n",
    "plt.ylabel('Nominal shift f(t) - f(0) [MHz]')\n",
    "plt.xlabel('Group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce61746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define values to classifier\n",
    "X = np.array([(time, shift_value, shift_std) for _, time, shift_value, shift_std in complete_list])\n",
    "y = np.array([group for group, _, _, _ in complete_list])\n",
    "\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "classifiers = {\n",
    "    \"Logistics Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    \"SVM (SVC)\": SVC(kernel='linear'),\n",
    "    \"SVM (RBF)\": SVC(kernel='rbf'),\n",
    "    \"SVM (Poly)\": SVC(kernel='poly')\n",
    "}\n",
    "\n",
    "# Metrics\n",
    "metrics_table = []\n",
    "\n",
    "# Run classifier\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # metrics calculate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # cross val\n",
    "    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Adicionar as métricas à tabela\n",
    "    metrics_table.append({\n",
    "        'Algorithm': name,\n",
    "        'CV Avg Accuracy': scores.mean(),\n",
    "        'CV Acc std dev': scores.std(),\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': report['weighted avg']['precision'],\n",
    "        'Recall': report['weighted avg']['recall'],\n",
    "        'F1-Score': report['weighted avg']['f1-score'],\n",
    "        'Support': report['weighted avg']['support'],\n",
    "    })\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    sns.heatmap(\n",
    "        conf_matrix, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues', \n",
    "        xticklabels=[\"control\", \"test\"], \n",
    "        yticklabels=[\"control\", \"test\"]\n",
    "    )\n",
    "    plt.title(f\"Confusion matrix - {name}\", fontsize=16)\n",
    "    plt.xlabel(\"Forseen\",fontsize=12)\n",
    "    plt.ylabel(\"True\",fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # print cross val metrics\n",
    "\n",
    "    metrics_table_export = [\n",
    "        ['Classifier', name],\n",
    "        ['Accuracy avg', scores.mean()]\n",
    "    ]\n",
    "    \n",
    "    print(tabulate(metrics_table_export))\n",
    "    print(f\"Accuracy 5 runs:       {scores}\")\n",
    "    print(\"\\n\")\n",
    "    print('-----------------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "# Converter a tabela em um DataFrame do Pandas\n",
    "metrics_df = pd.DataFrame(metrics_table)\n",
    "\n",
    "### Logging\n",
    "metrics_list_export = f'\\n {str(datetime.now())} - Metrics list - EXPORTED \\n'\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(metrics_list_export)\n",
    "    f.write(str(tabulate(metrics_table_export)))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = metrics_df.sort_values(by='CV Avg Accuracy', ascending=False)\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfebb18-5ad1-415f-b9bd-ab4ce58e7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logging\n",
    "all_metrics_df = tabulate(metrics_df, headers='keys', tablefmt='psql')\n",
    "\n",
    "all_metrics = f'{str(datetime.now())} - All metrics \\n' + str(all_metrics_df) + '\\n'\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(all_metrics)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd45bfe-33cb-4106-a2fa-e20da7ec5352",
   "metadata": {},
   "source": [
    "#### Model export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d5473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model_filename = folder + '/model_classifier.pkl'\n",
    "joblib.dump(clf, model_filename)\n",
    "\n",
    "### Logging\n",
    "model_file_export = f'\\n {str(datetime.now())} - Model file exported. Analysis is complete!'\n",
    "\n",
    "with open(log_file, 'a') as f:\n",
    "    f.write(model_file_export)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b89e0d-0b4c-4808-be81-a20c8796ea2e",
   "metadata": {},
   "source": [
    "#### Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e538741f-7422-4333-b251-01e8d5a894eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "txt_file_path_and_name = 'results_to_submit.csv'\n",
    "\n",
    "analysis_data = []\n",
    "\n",
    "with open(txt_file_path_and_name, mode='r', newline='') as csv_file:\n",
    "    reader_csv = csv.DictReader(csv_file)\n",
    "    for line in reader_csv:\n",
    "        analysis_data.append(line)\n",
    "\n",
    "analysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b34c9-ef44-4334-8fa1-824f70996c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_submit = []\n",
    "\n",
    "for item in analysis_data:\n",
    "    item_list = list(item.values())\n",
    "    data_to_submit.append(item_list)\n",
    "    \n",
    "data_to_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f6703-21a7-4962-9a00-d55469d9889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename_path = folder + '/model_classifier.pkl'\n",
    "loaded_model = joblib.load(model_filename_path)\n",
    "\n",
    "new_test_data = np.array(data_to_submit)\n",
    "prediction = loaded_model.predict(new_test_data)\n",
    "print(\"Prediction is :\", prediction[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
